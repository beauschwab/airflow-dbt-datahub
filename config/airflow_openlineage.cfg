# airflow.cfg — OpenLineage → DataHub Configuration
# ====================================================
# Add this section to your airflow.cfg or set equivalent
# AIRFLOW__OPENLINEAGE__* environment variables.

[openlineage]

# Transport: HTTP to DataHub GMS OpenLineage endpoint
transport = {
    "type": "http",
    "url": "https://datahub-gms.datahub.svc.cluster.local:8080/openapi/openlineage/",
    "endpoint": "api/v1/lineage",
    "auth": {
        "type": "api_key",
        "api_key": "${DATAHUB_API_TOKEN}"
    }
  }

# Namespace isolates this Airflow instance's lineage events
namespace = liquidity-pipelines-prod

# Auto-inject OpenLineage transport + parent job info into Spark jobs
# submitted via SparkKubernetesOperator.  This creates parent-child
# relationships: Airflow DAG → Spark job → Iceberg table in DataHub.
spark_inject_parent_job_info = true
spark_inject_transport_info = true

# Process OL events async (scheduler-side) for minimal task overhead
dag_state_change_process_pool_size = 2

# Include debug info (set to false in production for smaller events)
debug_mode = false
